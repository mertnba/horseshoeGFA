{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb94dd27b5c4bac0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_robustK(thrs, args, params, d_comps):\n",
    "    ## Initialize parameters\n",
    "\n",
    "    ncomps = args.K\n",
    "    nchs = args.num_chains\n",
    "    nsamples = args.num_samples\n",
    "    t_nsamples = nsamples * nchs\n",
    "    M, N, D = args.num_sources, params['Z'][0].shape[1], params['W'][0].shape[1]\n",
    "\n",
    "    # List of empty lists, one for each component, to store robust components.\n",
    "    X_rob = [[] for _ in range(ncomps)]\n",
    "\n",
    "    ## Initialization of Horseshoe Parameters for Z for all samples across all chains\n",
    "\n",
    "    Z = np.full([t_nsamples, N, ncomps], np.nan)\n",
    "\n",
    "    if args.model == 'sparseGFA':\n",
    "\n",
    "        # Initializes the local shrinkage parameters λ_Z\n",
    "        lmbZ = np.full([t_nsamples, N, ncomps], np.nan)\n",
    "\n",
    "        # Initializes the global shrinkage parameter τ_Z\n",
    "        tauZ = np.full([t_nsamples, ncomps], np.nan)\n",
    "\n",
    "        if args.reghsZ:\n",
    "            # Initializes the slab scale parameters cZ\n",
    "            cZ = np.full([t_nsamples, ncomps], np.nan)\n",
    "\n",
    "            ## Initialization of Horseshoe Parameters for W for all samples across all chains\n",
    "\n",
    "    W = np.full([t_nsamples, D, ncomps], np.nan)\n",
    "\n",
    "    if 'sparseGFA' in args.model:\n",
    "\n",
    "        # Initializes the local shrinkage parameters λ_W\n",
    "        lmbW = np.full([t_nsamples, D, ncomps], np.nan)\n",
    "\n",
    "        # Initializes the slab scale parameters cW\n",
    "        cW = np.full([t_nsamples, M, ncomps], np.nan)\n",
    "\n",
    "\n",
    "    elif args.model == 'GFA':\n",
    "\n",
    "        alpha = np.full([t_nsamples, M, ncomps], np.nan)\n",
    "\n",
    "        ## Component Selection Initialization\n",
    "\n",
    "    # Initializes a list to store component indices for each chain: a list with dimension: number of chains x number of components\n",
    "    storecomps = [np.arange(ncomps) for _ in range(nchs)]\n",
    "\n",
    "    # Retrieves the cosine similarity threshold (minimum similarity required for components to be considered as matching (same factors) across different chains)\n",
    "    cosThr = thrs['cosineThr']\n",
    "\n",
    "    # Retrieves the match threshold (the minimum number of chains in which a component must be present to be considered robust)\n",
    "    matchThr = thrs['matchThr']\n",
    "\n",
    "    # Initializes a counter for the number of robust components identified\n",
    "    nrobcomp = 0\n",
    "\n",
    "    ## Component Matching Across Chains\n",
    "\n",
    "    # Iterates over each chain\n",
    "    for c1 in range(nchs):\n",
    "\n",
    "        # Matrix to store the similarity of each component with components in each chain.\n",
    "        max_sim = np.zeros((ncomps, nchs))\n",
    "\n",
    "        # Matrix to store maximum cosine similarities for the loading matrices W.\n",
    "        max_simW = np.zeros((ncomps, nchs))\n",
    "\n",
    "        # Matrix to store maximum cosine similarities for the latent variables Z.\n",
    "        max_simZ = np.zeros((ncomps, nchs))\n",
    "\n",
    "        # Initializes an array to store the matching indices for each component and chain.\n",
    "        matchInds = np.zeros((ncomps, nchs))\n",
    "\n",
    "        # Iterates over each component k in the current chain c1, the goal is to find matching components in other chains\n",
    "        for k in storecomps[c1]:\n",
    "\n",
    "            # Initializes an empty list to store chains that have components left to be matched.\n",
    "            nonempty_chs = []\n",
    "\n",
    "            # This loop identifies chains that have non-empty components and appends them to nonempty_chs.\n",
    "            for ne in range(len(storecomps)):\n",
    "                if storecomps[ne].size > 0:\n",
    "                    nonempty_chs.append(ne)\n",
    "\n",
    "            # Iterates over each non-empty chain c2 to find the most similar components to k.\n",
    "            for c2 in nonempty_chs:\n",
    "\n",
    "                # Initializes an array to store cosine similarities between component \"k\" in chain \"c1\" and each component in chain \"c2\".\n",
    "                cosine = np.zeros((1, storecomps[c2].size))\n",
    "\n",
    "                # Initializes an array to store cosine similarities for the loading matrices W.\n",
    "                cosW = np.zeros((1, storecomps[c2].size))\n",
    "\n",
    "                # Initializes an array to store cosine similarities for the latent variables Z.\n",
    "                cosZ = np.zeros((1, storecomps[c2].size))\n",
    "\n",
    "                # Initializes an index for storing similarities in the arrays\n",
    "                cind = 0\n",
    "\n",
    "                ## Calculating Cosine Similarities\n",
    "\n",
    "                # Iterates over each component in chain \"c2\", to compare with the current component \"k\" in \"c1\" to find the most similar one.\n",
    "                for comp in storecomps[c2]:\n",
    "\n",
    "                    \"\"\"\n",
    "                    comp1 = np.ndarray.flatten(d_comps[c2][comp])\n",
    "                    comp2 = np.ndarray.flatten(d_comps[c1][k])\n",
    "                    cosine[0, cind] = np.dot(comp1, comp2) / (np.linalg.norm(comp1) * np.linalg.norm(comp2))\n",
    "\n",
    "                    compW1 = np.mean(params['W'][c2], axis=0)[:, comp]\n",
    "                    compW2 = np.mean(params['W'][c1], axis=0)[:, k]\n",
    "                    cosW[0, cind] = np.dot(compW1, compW2) / (np.linalg.norm(compW1) * np.linalg.norm(compW2))\n",
    "\n",
    "                    compZ1 = np.mean(params['Z'][c2], axis=0)[:, comp]\n",
    "                    compZ2 = np.mean(params['Z'][c1], axis=0)[:, k]\n",
    "                    cosZ[0, cind] = np.dot(compZ1, compZ2) / (np.linalg.norm(compZ1) * np.linalg.norm(compZ2))\n",
    "                    \"\"\"\n",
    "\n",
    "                    ## X\n",
    "                    comp1 = np.ndarray.flatten(d_comps[c2][comp])\n",
    "                    comp2 = np.ndarray.flatten(d_comps[c1][k])\n",
    "                    norm1_X = np.linalg.norm(comp1)\n",
    "                    norm2_X = np.linalg.norm(comp2)\n",
    "                    if norm1_X == 0 or norm2_X == 0:\n",
    "                        cosine[0, cind] = 0\n",
    "                    else:\n",
    "                        cosine[0, cind] = np.dot(comp1, comp2) / (norm1_X * norm2_X)\n",
    "\n",
    "                    ## W\n",
    "                    compW1 = np.mean(params['W'][c2], axis=0)[:, comp]\n",
    "                    compW2 = np.mean(params['W'][c1], axis=0)[:, k]\n",
    "                    norm1_W = np.linalg.norm(compW1)\n",
    "                    norm2_W = np.linalg.norm(compW2)\n",
    "                    if norm1_W == 0 or norm2_W == 0:\n",
    "                        cosW[0, cind] = 0\n",
    "                    else:\n",
    "                        cosW[0, cind] = np.dot(compW1, compW2) / (norm1_W * norm2_W)\n",
    "\n",
    "                    ## Z\n",
    "                    compZ1 = np.mean(params['Z'][c2], axis=0)[:, comp]\n",
    "                    compZ2 = np.mean(params['Z'][c1], axis=0)[:, k]\n",
    "                    norm1_Z = np.linalg.norm(compZ1)\n",
    "                    norm2_Z = np.linalg.norm(compZ2)\n",
    "                    if norm1_Z == 0 or norm2_Z == 0:\n",
    "                        cosZ[0, cind] = 0\n",
    "                    else:\n",
    "                        cosZ[0, cind] = np.dot(compZ1, compZ2) / (norm1_Z * norm2_Z)\n",
    "\n",
    "                    cind += 1\n",
    "\n",
    "                ## Identifying the Most Similar Components\n",
    "\n",
    "                # Finds the maximum cosine similarity for component \"k\" in chain \"c1\" with the components in chain \"c2\".\n",
    "                max_sim[k, c2] = cosine[0, np.argmax(cosine)]\n",
    "\n",
    "                max_simW[k, c2] = cosW[0, np.argmax(cosine)]\n",
    "\n",
    "                max_simZ[k, c2] = cosZ[0, np.argmax(cosine)]\n",
    "\n",
    "                # Records the index of the component in \"c2\" that is most similar to the component \"k\" in \"c1\".\n",
    "                matchInds[k, c2] = storecomps[c2][np.argmax(cosine)]\n",
    "\n",
    "                # Sets the match index to -1 if the maximum similarity is below the cosine threshold.\n",
    "                if max_sim[k, c2] < cosThr:\n",
    "                    matchInds[k, c2] = -1\n",
    "\n",
    "            ## Identifying Robust Components\n",
    "\n",
    "            # Checks if the number of chains with a similarity above the threshold exceeds the match threshold.\n",
    "            if np.sum(max_sim[k, :] > cosThr) > matchThr * nchs:\n",
    "\n",
    "                # Finds indices of chains where the component k has a match.\n",
    "                goodInds = np.where(matchInds[k, :] >= 0)\n",
    "\n",
    "                # Initializes an array to store the robust component k.\n",
    "                X_rob[k] = np.zeros((N, D))\n",
    "\n",
    "                # Initializes a sample index counter.\n",
    "                s = 0\n",
    "\n",
    "                # Loops over the chains where the component k has a match.\n",
    "                for c2 in list(goodInds[0]):\n",
    "\n",
    "                    # Creates an array of sample indices.\n",
    "                    inds = np.arange(s, s + nsamples)\n",
    "\n",
    "                    # Adds the matched component data to X_rob[k].\n",
    "                    X_rob[k] += d_comps[c2][int(matchInds[k, c2])]\n",
    "\n",
    "                    ## Updating Parameters for Robust Components\n",
    "\n",
    "                    if args.model == 'sparseGFA':\n",
    "\n",
    "                        # Updates the local shrinkage parameters for Z.\n",
    "                        lmbZ[inds, :, k] = params['lmbZ'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                        # Updates the global shrinkage parameters for Z.\n",
    "                        tauZ[inds, k] = params['tauZ'][c2][:, int(matchInds[k, c2])]\n",
    "\n",
    "                        # Updates the slab scale parameters for Z if regularized horseshoe priors are used.\n",
    "                        if args.reghsZ:\n",
    "                            cZ[inds, k] = params['cZ'][c2][:, int(matchInds[k, c2])]\n",
    "\n",
    "                    if 'sparseGFA' in args.model:\n",
    "\n",
    "                        # Updates the local shrinkage parameters for W\n",
    "                        lmbW[inds, :, k] = params['lmbW'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                        # Updates the slab scale parameters for W.\n",
    "                        cW[inds, :, k] = params['cW'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                    # Updates the ARD parameters for the GFA model.\n",
    "                    elif args.model == 'GFA':\n",
    "                        alpha[inds, :, k] = params['alpha'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                    ## Handling Loading Matrices and Latent Variables\n",
    "\n",
    "                    # Updates the loading matrices if similarity is positive.\n",
    "                    if max_simW[k, c2] > 0:\n",
    "                        W[inds, :, k] = params['W'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                    # Inverts the loading matrices if similarity is negative.\n",
    "                    else:\n",
    "                        W[inds, :, k] = -params['W'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                    # Updates the latent variables if similarity is positive.\n",
    "                    if max_simZ[k, c2] > 0:\n",
    "                        Z[inds, :, k] = params['Z'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                    # Inverts the latent variables if similarity is negative.\n",
    "                    else:\n",
    "                        Z[inds, :, k] = -params['Z'][c2][:, :, int(matchInds[k, c2])]\n",
    "\n",
    "                    ## Updating Components and Sample Indices\n",
    "\n",
    "                    # Updates storecomps[c2] by removing the index of the matching component (int(matchInds[k, c2])) for component k.\n",
    "                    storecomps[c2] = storecomps[c2][storecomps[c2] != int(matchInds[k, c2])]\n",
    "\n",
    "                    s += nsamples\n",
    "\n",
    "                # Averages the robust component data.\n",
    "                X_rob[k] = [X_rob[k] / np.sum(matchInds[k, :] >= 0)]\n",
    "\n",
    "                nrobcomp += 1\n",
    "\n",
    "    success = True\n",
    "\n",
    "    ## Removal of Non-Robust Components\n",
    "\n",
    "    if nrobcomp > 0:\n",
    "\n",
    "        # Identifies columns in Z that are not entirely NaN. This step filters out non-robust components by ensuring only columns with valid data are retained.\n",
    "        idx_cols = ~np.isnan(np.mean(Z, axis=1)).all(axis=0)\n",
    "\n",
    "        # idx_cols: A boolean array indicating which columns (components) are robust (not all NaN).\n",
    "\n",
    "        # Identifies rows in Z that do not have any NaN values in the retained columns. Ensures that only valid rows are kept, filtering out samples with missing data.\n",
    "        idx_rows = ~np.isnan(np.mean(Z, axis=1)[:, idx_cols]).any(axis=1)\n",
    "\n",
    "        # idx_rows: A boolean array indicating which rows (samples) are robust (not containing any NaN in the robust components).\n",
    "\n",
    "        if args.model == 'sparseGFA':\n",
    "\n",
    "            lmbZ = lmbZ[idx_rows, :, :]\n",
    "            lmbZ = lmbZ[:, :, idx_cols]\n",
    "\n",
    "            tauZ = tauZ[idx_rows, :]\n",
    "            tauZ = tauZ[:, idx_cols]\n",
    "\n",
    "            if args.reghsZ:\n",
    "\n",
    "                cZ = cZ[idx_rows, :]\n",
    "                cZ = cZ[:, idx_cols]\n",
    "\n",
    "                if cZ.size == 0:\n",
    "                    print('No samples survived!')\n",
    "                    success = False\n",
    "\n",
    "        W = W[idx_rows, :, :]\n",
    "        W = W[:, :, idx_cols]\n",
    "\n",
    "        Z = Z[idx_rows, :, :]\n",
    "        Z = Z[:, :, idx_cols]\n",
    "\n",
    "        if 'sparseGFA' in args.model:\n",
    "\n",
    "            lmbW = lmbW[idx_rows, :, :]\n",
    "            lmbW = lmbW[:, :, idx_cols]\n",
    "\n",
    "            cW = cW[idx_rows, :, :]\n",
    "            cW = cW[:, :, idx_cols]\n",
    "\n",
    "        elif args.model == 'GFA':\n",
    "\n",
    "            alpha = alpha[idx_rows, :, :]\n",
    "            alpha = alpha[:, :, idx_cols]\n",
    "\n",
    "        # Removes empty entries from X_rob.\n",
    "        X_rob_final = [X_rob[i] for i in range(len(X_rob)) if X_rob[i] != []]\n",
    "        X_rob = X_rob_final\n",
    "\n",
    "\n",
    "    else:\n",
    "        print('No robust components found!')\n",
    "        success = False\n",
    "\n",
    "        ## Creating Dictionary of Robust Parameters (save the posterior mean for most parameters)\n",
    "\n",
    "    rob_params = {'W': np.mean(W, axis=0),\n",
    "                  'Z': np.mean(Z, axis=0)}  # np.mean(W, axis=0) - Mean of W parameters across samples\n",
    "\n",
    "    if 'sparseGFA' in args.model:\n",
    "        rob_params.update({'cW_inf': np.mean(cW, axis=0), 'lmbW': np.mean(lmbW, axis=0)})\n",
    "\n",
    "    elif args.model == 'GFA':\n",
    "        rob_params.update({'alpha_inf': np.mean(alpha, axis=0)})\n",
    "\n",
    "    if args.model == 'sparseGFA':\n",
    "\n",
    "        if args.reghsZ:\n",
    "            rob_params.update({'cZ_inf': cZ, 'tauZ_inf': tauZ, 'lmbZ': np.mean(lmbZ, axis=0)})\n",
    "\n",
    "        else:\n",
    "            rob_params.update({'tauZ_inf': tauZ, 'lmbZ': np.mean(lmbZ, axis=0)})\n",
    "\n",
    "    return rob_params, X_rob, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_infparams(samples, hypers, args):\n",
    "\n",
    "    ## Inferred parameters\n",
    "\n",
    "    nchs, nsamples = args.num_chains, args.num_samples\n",
    "    N = samples['Z'].shape[1]\n",
    "    K = args.K\n",
    "\n",
    "    ## Initialization of Latent Variables and Parameters for Z\n",
    "\n",
    "    # List of arrays to store inferred Z parameters for each chain (posterior samples of the Z parameters for each chain)\n",
    "    Z_inf = [np.zeros((nsamples, N, K)) for _ in range(nchs)]\n",
    "\n",
    "    if args.model == 'sparseGFA':\n",
    "\n",
    "        # List of arrays to store inferred local shrinkage parameters (λ_Z) for latent variables for each chain.\n",
    "        lmbZ_inf = [np.zeros((nsamples, N, K)) for _ in range(nchs)]\n",
    "\n",
    "        # List of arrays to store inferred global shrinkage parameters (τ_Z) parameters for each chain.\n",
    "        tauZ_inf = [np.zeros((nsamples, K)) for _ in range(nchs)]\n",
    "\n",
    "        if args.reghsZ:\n",
    "            # List of arrays to store inferred slab scale parameters (cZ) for each chain.\n",
    "            cZ_inf = [np.zeros((nsamples, K)) for _ in range(nchs)]\n",
    "\n",
    "    ## Initialization of Loading Matrices and Parameters for W\n",
    "\n",
    "    D = sum(hypers['Dm'])\n",
    "\n",
    "    print(f\"Dm array: {hypers['Dm']}\")\n",
    "\n",
    "    # List of arrays to store inferred loading matrices (W) for each chain.\n",
    "    W_inf = [np.zeros((nsamples, D, K)) for _ in range(nchs)]\n",
    "\n",
    "    if 'sparseGFA' in args.model:\n",
    "\n",
    "        # List of arrays to store inferred local shrinkage parameters (λ_W) for each chain.\n",
    "        lmbW_inf = [np.zeros((nsamples, D, K)) for _ in range(nchs)]\n",
    "\n",
    "        # List of arrays to store inferred slab scale parameters (cW) for each chain.\n",
    "        cW_inf = [np.zeros((nsamples, K)) for _ in range(nchs)]\n",
    "\n",
    "        # Array to store inferred global shrinkage parameters (τ_W) across all samples and chains.\n",
    "        tauW_inf = np.zeros((nchs * nsamples, args.num_sources))\n",
    "\n",
    "    elif args.model == 'GFA':\n",
    "\n",
    "        # List of arrays to store inferred ARD parameters (α) for each chain.\n",
    "        alpha_inf = [np.zeros((nsamples, K)) for _ in range(nchs)]\n",
    "\n",
    "    ## Initialization of Noise Parameters\n",
    "\n",
    "    # Array to store inferred noise parameters (σ) across all samples and chains.\n",
    "    sigma_inf = np.zeros((nchs * nsamples, args.num_sources))\n",
    "\n",
    "    ## Initialization of d_comps\n",
    "\n",
    "    # List of lists to store data components for each chain and component. For each chain (outer list) and each component (inner list), an empty list is initialized.\n",
    "    d_comps = [[[] for _ in range(K)] for _ in range(nchs)]\n",
    "\n",
    "    ## Iteration Over Chains and Sample Index Management\n",
    "\n",
    "    s = 0\n",
    "\n",
    "    for c in range(nchs):\n",
    "        inds = np.arange(s, s + nsamples)\n",
    "\n",
    "        # Extracting sigma:\n",
    "        sigma_inf[inds, :] = samples['sigma'][inds, 0, :]\n",
    "\n",
    "        ## Processing Inferred Parameters for Z\n",
    "\n",
    "        # Iterates over each component to process the inferred parameters for Z.\n",
    "        for k in range(K):\n",
    "\n",
    "            if args.model == 'sparseGFA':\n",
    "\n",
    "                # Extracts and stores the inferred global shrinkage parameter (τ_Z) for the current chain and component.\n",
    "                tauZ_inf[c][:, k] = np.array(samples[f'tauZ'])[inds, 0, k]\n",
    "\n",
    "                # Reshapes the τ_Z array to ensure correct dimensional alignment for subsequent operations.\n",
    "                tauZ = np.reshape(tauZ_inf[c][:, k], (nsamples, 1))\n",
    "\n",
    "                if args.reghsZ:\n",
    "\n",
    "                    # Calculates the slab scale parameter (cZ).\n",
    "                    cZ_inf[c][:, k] = hypers['slab_scale'] * np.sqrt(samples['cZ'][inds, 0, k])\n",
    "\n",
    "                    # Reshapes the cZ array for correct dimensional alignment.\n",
    "                    cZ = np.reshape(cZ_inf[c][:, k], (nsamples, 1))\n",
    "\n",
    "                    # Squares the local shrinkage parameter (λ_Z) values for the current chain and component.\n",
    "                    lmbZ_sqr = np.square(np.array(samples['lmbZ'])[inds, :, k])\n",
    "\n",
    "                    # Calculates the adjusted local shrinkage parameter using the horseshoe prior formula.\n",
    "                    lmbZ_inf[c][:, :, k] = np.sqrt(lmbZ_sqr * cZ ** 2 / (cZ ** 2 + tauZ ** 2 * lmbZ_sqr))\n",
    "\n",
    "                else:\n",
    "                    lmbZ_inf[c][:, :, k] = np.array(samples['lmbZ'])[inds, :, k]\n",
    "\n",
    "                #  Calculate the final latent variables by combining Z, λ_Z, and τ_Z.\n",
    "                Z_inf[c][:, :, k] = np.array(samples['Z'])[inds, :, k] * lmbZ_inf[c][:, :, k] * tauZ\n",
    "\n",
    "            else:\n",
    "                # Directly extract the latent variables without additional shrinkage adjustments.\n",
    "                Z_inf[c][:, :, k] = np.array(samples['Z'])[inds, :, k]\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"Z_inf for chain {c}\")\n",
    "        print(Z_inf)\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        ## Processing Inferred Parameters for W for the Sparse GFA model\n",
    "\n",
    "        if 'sparseGFA' in args.model:\n",
    "\n",
    "            # Calculates the slab scale parameter (cW) for the loading matrices.\n",
    "            cW_inf[c] = hypers['slab_scale'] * np.sqrt(samples['cW'][inds, :, :])\n",
    "\n",
    "            Dm = hypers['Dm']\n",
    "\n",
    "            d = 0\n",
    "\n",
    "            for m in range(args.num_sources):\n",
    "\n",
    "                # Squares the local shrinkage parameter (λ_W) values for the current data source and component.\n",
    "                lmbW_sqr = np.square(np.array(samples['lmbW'][inds, d:d + Dm[m], :]))\n",
    "\n",
    "                # Extracts and stores the inferred global shrinkage parameter (τ_W) for the current data source.\n",
    "                tauW_inf[inds, m] = samples[f'tauW{m + 1}'][inds]\n",
    "\n",
    "                # Reshapes the τ_W array for correct dimensional alignment.\n",
    "                tauW = np.reshape(tauW_inf[inds, m], (nsamples, 1, 1))\n",
    "\n",
    "                # Extracts and Reshapes the cW array for correct dimensional alignment.\n",
    "                cW = np.reshape(cW_inf[c][:, m, :], (nsamples, 1, K))\n",
    "\n",
    "                # Calculates the adjusted local shrinkage parameter using the horseshoe prior formula.\n",
    "                lmbW_inf[c][:, d:d+Dm[m], :] = np.sqrt(cW ** 2 * lmbW_sqr / (cW ** 2 + tauW ** 2 * lmbW_sqr))\n",
    "\n",
    "\n",
    "                # Calculates the final loading matrices by combining W, λ_W, and τ_W.\n",
    "                W_inf[c][:, d:d + Dm[m], :] = np.array(samples['W'][inds, d:d + Dm[m], :]) * lmbW_inf[c][:, d:d + Dm[m], :] * tauW\n",
    "\n",
    "                # Updates the feature index counter.\n",
    "                d += Dm[m]\n",
    "\n",
    "            \"\"\"\n",
    "            print(f\"cW_inf for chain {c}\")\n",
    "            print(cW_inf)\n",
    "            print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            print(f\"lmbW_inf for chain {c}\")\n",
    "            print(lmbW_inf)\n",
    "            print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            print(f\"tauW_inf for chain {c}\")\n",
    "            print(tauW_inf)\n",
    "            print(\"------------------------------------------------------------------------------------------\")\n",
    "            \"\"\"\n",
    "\n",
    "        ## Processing Inferred Parameters for GFA\n",
    "\n",
    "        elif args.model == 'GFA':\n",
    "\n",
    "            alpha_inf[c] = samples['alpha'][inds, :, :]\n",
    "\n",
    "            Dm = hypers['Dm']\n",
    "\n",
    "            d = 0\n",
    "\n",
    "            for m in range(args.num_sources):\n",
    "                alpha = np.reshape(alpha_inf[c][:, m, :], (nsamples, 1, K))\n",
    "\n",
    "                W_inf[c][:, d:d + Dm[m], :] = np.array(samples['W'][inds, d:d + Dm[m], :]) * (1 / np.sqrt(alpha))\n",
    "\n",
    "                d += Dm[m]\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"samples['W'] for chain {c}\")\n",
    "        print(samples['W'])\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(f\"W_inf for chain {c}\")\n",
    "        print(W_inf)\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        ## Computation of Components in the Data Space\n",
    "\n",
    "        # Computes the data components for each component k by taking the mean of the inferred Z and W matrices across samples and then computing their dot product.\n",
    "        for k in range(K):\n",
    "            # Computes the mean of the Z values across samples for the current chain c and component k, and reshapes it into a column vector.\n",
    "            z = np.reshape(np.mean(Z_inf[c][:, :, k], axis=0), (N, 1))\n",
    "\n",
    "            # Computes the mean of the W values across samples for the current chain c and component k, and reshapes it into a column vector.\n",
    "            w = np.reshape(np.mean(W_inf[c][:, :, k], axis=0), (D, 1))\n",
    "\n",
    "            # Computes the data components by taking the dot product of z and the transpose of w.\n",
    "            # Combines latent variables and loading matrices to generate the components in data space, representing the reconstructed data for the current component.\n",
    "            d_comps[c][k] = np.dot(z, w.T)\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"d_comps for chain {c}\")\n",
    "        print(d_comps)\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        s += nsamples\n",
    "\n",
    "    ## Consolidation of Results\n",
    "\n",
    "    params = {'W': W_inf, 'Z': Z_inf, 'sigma': sigma_inf}\n",
    "\n",
    "    if 'sparseGFA' in args.model:\n",
    "        params.update({'lmbW': lmbW_inf, 'tauW': tauW_inf, 'cW': cW_inf})\n",
    "    elif args.model == 'GFA':\n",
    "        params.update({'alpha': alpha_inf})\n",
    "\n",
    "    if args.model == 'sparseGFA':\n",
    "        if args.reghsZ:\n",
    "            params.update({'lmbZ': lmbZ_inf, 'tauZ': tauZ_inf, 'cZ': cZ_inf})\n",
    "        else:\n",
    "            params.update({'lmbZ': lmbZ_inf, 'tauZ': tauZ_inf})\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Final params and d_comps\")\n",
    "    print(params)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"Final d_comps\")\n",
    "    print(d_comps)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Returns the dictionary of inferred parameters and computed components in the data space: a list of lists, with each sublist containing arrays of shape [N, D])\n",
    "    return params, d_comps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39d0c66f8e567103"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
