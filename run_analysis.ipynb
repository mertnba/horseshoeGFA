{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler\n",
    "import get_data\n",
    "import visualization\n",
    "from utils import get_infparams, get_robustK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def models(X, hypers, args):\n",
    "\n",
    "    ## Initialize parameters:\n",
    "      \n",
    "    N, M, Dm = X.shape[0], args.num_sources, hypers['Dm']  \n",
    "    D, K, percW = sum(Dm), args.K, hypers['percW']\n",
    "\n",
    "\n",
    "    ## Sampling sigma:\n",
    "    \n",
    "    sigma = numpyro.sample(\"sigma\", dist.Gamma(hypers['a_sigma'], hypers['b_sigma']), sample_shape=(1, M))\n",
    "\n",
    "\n",
    "    ## Sampling Z:\n",
    "\n",
    "    if args.model == 'sparseGFA':\n",
    "            \n",
    "        # Sampling Z for Sparse GFA\n",
    "        Z = numpyro.sample(\"Z\", dist.Normal(0, 1), sample_shape=(N, K))\n",
    "    \n",
    "        # Sampling tauZ for Sparse GFA\n",
    "        tauZ = numpyro.sample(f'tauZ', dist.TruncatedCauchy(scale=1), sample_shape=(1, K))\n",
    "\n",
    "        # Sampling lambdaZ for Sparse GFA\n",
    "        lmbZ = numpyro.sample(\"lmbZ\", dist.TruncatedCauchy(scale=1), sample_shape=(N, K))\n",
    "    \n",
    "        if args.reghsZ:   \n",
    "            \n",
    "            # Sampling cZ for Regularized Horseshoe Prior\n",
    "            cZtmp = numpyro.sample(\"cZ\", dist.InverseGamma(0.5 * hypers['slab_df'], 0.5 * hypers['slab_df']), sample_shape=(1, K))\n",
    "            \n",
    "            # Transforming cZ\n",
    "            cZ = hypers['slab_scale'] * jnp.sqrt(cZtmp)\n",
    "            \n",
    "            # Computing Regularized Z\n",
    "            lmbZ_sqr = jnp.square(lmbZ)\n",
    "            \n",
    "            # Iterate over each latent factor\n",
    "            for k in range(K):    \n",
    "                \n",
    "                # Calculate the adjusted local shrinkage parameter for each component k.\n",
    "                lmbZ_tilde = jnp.sqrt( lmbZ_sqr[:, k] * cZ[0, k] ** 2 / ( cZ[0, k] ** 2 + tauZ[0, k] ** 2 * lmbZ_sqr[:, k] ) )\n",
    "\n",
    "                # Updates Z by multiplying with lmbZ_tilde and tauZ for each component k.\n",
    "                Z = Z.at[:, k].set( Z[:, k] * lmbZ_tilde * tauZ[0, k] )\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            Z = Z * lmbZ * tauZ\n",
    "\n",
    "    else:\n",
    "       \n",
    "        Z = numpyro.sample(\"Z\", dist.Normal(0, 1), sample_shape=(N, K))\n",
    "\n",
    "\n",
    "\n",
    "    ## Sampling W:\n",
    "    \n",
    "    W = numpyro.sample(\"W\", dist.Normal(0, 1), sample_shape=(D, K))\n",
    "\n",
    "    if 'sparseGFA' in args.model:\n",
    "\n",
    "        # Sampling lambdaW for Sparse GFA\n",
    "        lmbW = numpyro.sample(\"lmbW\", dist.TruncatedCauchy(scale=1), sample_shape=(D, K))\n",
    "        \n",
    "        # Sampling cW for Regularized Horseshoe Prior on W\n",
    "        cWtmp = numpyro.sample(\"cW\", dist.InverseGamma(0.5 * hypers['slab_df'], 0.5 * hypers['slab_df']), sample_shape=(M, K))\n",
    "        \n",
    "        # Transforming cW\n",
    "        cW = hypers['slab_scale'] * jnp.sqrt(cWtmp)\n",
    "\n",
    "        # Computing pW\n",
    "        pW = np.round((percW/100) * Dm)\n",
    "\n",
    "        d = 0   \n",
    "            \n",
    "        # Loop Over Each Modality to Apply Regularized Horseshoe Prior on W\n",
    "        for m in range(M): \n",
    "            \n",
    "            # Computes the scale parameter for the global shrinkage parameters tauW.\n",
    "            scaleW = pW[m] / ((Dm[m] - pW[m]) * jnp.sqrt(N)) \n",
    "            \n",
    "            # Samples tauW for the current modality.\n",
    "            tauW = numpyro.sample(f'tauW{m+1}', dist.TruncatedCauchy(scale=scaleW * 1/jnp.sqrt(sigma[0, m])))\n",
    "\n",
    "            # Reshapes the squared local shrinkage parameters to match the dimensions of the current modality.\n",
    "            lmbW_sqr = jnp.reshape( jnp.square( lmbW[d:d+Dm[m], :] ), ( Dm[m], K ) )\n",
    "            \n",
    "            # Calculates the adjusted local shrinkage parameter.\n",
    "            lmbW_tilde = jnp.sqrt( cW[m, :] ** 2 * lmbW_sqr / ( cW[m, :] ** 2 + tauW ** 2 * lmbW_sqr ) )   \n",
    "            \n",
    "            # Updates the weight matrix W for the current modality.\n",
    "            W = W.at[d:d+Dm[m], :].set(W[d:d+Dm[m], :] * lmbW_tilde * tauW)\n",
    "            \n",
    "            # Sampling X for Each Modality (part of the generative process of the GFA model)   \n",
    "            numpyro.sample( f'X{m+1}', dist.Normal( jnp.dot( Z, W[d:d+Dm[m], :].T ), 1/jnp.sqrt( sigma[0, m] ) ), obs=X[:, d:d+Dm[m]] )\n",
    "            \n",
    "            d += Dm[m]\n",
    "\n",
    "\n",
    "    elif args.model == 'GFA':\n",
    "        alpha = numpyro.sample(\"alpha\", dist.Gamma(1e-3, 1e-3), sample_shape=(M, K))\n",
    "        d = 0\n",
    "        for m in range(M):\n",
    "            W = W.at[d:d+Dm[m], :].set(W[d:d+Dm[m], :] * (1/jnp.sqrt(alpha[m, :])))\n",
    "            numpyro.sample(f'X{m+1}', dist.Normal(jnp.dot(Z, W[d:d+Dm[m], :].T), 1/jnp.sqrt(sigma[0, m])), obs=X[:, d:d+Dm[m]])\n",
    "            d += Dm[m]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "788ca364ca9a93c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_inference(model, args, rng_key, X, hypers):\n",
    "\n",
    "    kernel = NUTS(model)\n",
    "\n",
    "    mcmc = MCMC(kernel, num_warmup=args.num_warmup, num_samples=args.num_samples, num_chains=args.num_chains)\n",
    "\n",
    "    mcmc.run(rng_key, X, hypers, args, extra_fields=('potential_energy',))\n",
    "\n",
    "    # mcmc.print_summary()\n",
    "\n",
    "    return mcmc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58959c32d0f8e57b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(args):   \n",
    "    \n",
    "    ## Directory Setup for Results\n",
    "    \n",
    "    if 'synthetic' in args.dataset:                                                                       \n",
    "        flag = f'K{args.K}_{args.num_chains}chs_pW{args.percW}_s{args.num_samples}_addNoise{args.noise}'\n",
    "\n",
    "    else:                                                                                               \n",
    "        flag = f'K{args.K}_{args.num_chains}chs_pW{args.percW}_s{args.num_samples}'                       \n",
    "\n",
    "    if args.model == 'sparseGFA':                                           \n",
    "        if args.reghsZ:                                                   \n",
    "            flag_regZ = '_reghsZ'                                         \n",
    "        else:                                                               \n",
    "            flag_regZ = '_hsZ'                                    \n",
    "    else:                                                          \n",
    "        flag_regZ = ''                                           \n",
    "\n",
    "    res_dir = f'/Users/mertenbiyaoglu/Desktop/ucl/thesis/codes/sGFA_AIDA/results/{args.dataset}/{args.model}_{flag}{flag_regZ}'\n",
    "    \n",
    "    if not os.path.exists(res_dir):                                 \n",
    "        os.makedirs(res_dir)                                         \n",
    "\n",
    "\n",
    "\n",
    "    ## Setting Up Hyperparameters\n",
    "        \n",
    "    hp_path = f'{res_dir}/hyperparameters.dictionary'                      \n",
    "    \n",
    "    if not os.path.exists(hp_path):                                        \n",
    "        hypers = {'a_sigma': 1, 'b_sigma': 1, 'nu_local': 1, 'nu_global': 1, 'slab_scale': 2, 'slab_df': 4, 'percW': args.percW}\n",
    "        \n",
    "        with open(hp_path, 'wb') as parameters:                             \n",
    "            pickle.dump(hypers, parameters)                               \n",
    "    \n",
    "    else:                                                             \n",
    "        with open(hp_path, 'rb') as parameters:                           \n",
    "            hypers = pickle.load(parameters)                              \n",
    "\n",
    "\n",
    "\n",
    "    ## Initializing and Loading Data for Each Run\n",
    "    \n",
    "    for i in range(args.num_runs):                                          \n",
    "        \n",
    "        print('Initialisation: ', i+1)\n",
    "        print('----------------------------------')\n",
    "        \n",
    "        if 'synthetic' in args.dataset:                                     \n",
    "            data_path = f'{res_dir}/[{i+1}]Data.dictionary'               \n",
    "\n",
    "            if not os.path.exists(data_path):                              \n",
    "                data = get_data.synthetic_data(hypers, args)               \n",
    "                \n",
    "                with open(data_path, 'wb') as parameters:                  \n",
    "                    pickle.dump(data, parameters)                           \n",
    "            \n",
    "            else:                                                           \n",
    "                with open(data_path, 'rb') as parameters:                 \n",
    "                    data = pickle.load(parameters)                   \n",
    "            \n",
    "            X = data['X']                                                  \n",
    "            \n",
    "        \n",
    "        elif 'genfi' in args.dataset:\n",
    "\n",
    "            datafolder = f'./aida_model'\n",
    "\n",
    "            data = get_data.genfi(datafolder)\n",
    "\n",
    "            X = data['X'].copy()\n",
    "            Y = data['Y']\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            robust_scaler = RobustScaler()\n",
    "            transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "            # Modality Slices\n",
    "            X1_columns = slice(0, 33)\n",
    "            X2_columns = slice(33, 62)\n",
    "            X3_columns = slice(62, 68)\n",
    "            X4_columns = slice(68, 108)\n",
    "                \n",
    "            # Standard Scaler (Feature-wise) and Box-Cox if \n",
    "            X[:, X1_columns] = transformer.fit_transform(scaler.fit_transform(X[:, X1_columns]))\n",
    "            X[:, X2_columns] = transformer.fit_transform(scaler.fit_transform(X[:, X2_columns]))\n",
    "            X[:, X3_columns] = transformer.fit_transform(scaler.fit_transform(X[:, X3_columns]))\n",
    "            X[:, X4_columns] = transformer.fit_transform(scaler.fit_transform(X[:, X4_columns]))\n",
    "\n",
    "            \n",
    "        hypers.update({'Dm': data.get('Dm')})\n",
    "        \n",
    "                                            \n",
    "    ## Running the Model, Handling Robust Parameter Extraction, and Saving Results \n",
    "        \n",
    "        res_path = f'{res_dir}/[{i+1}]Model_params.dictionary'              # Constructs the path to the file where all the model parameters are saved.\n",
    "    \n",
    "        robparams_path = f'{res_dir}/[{i+1}]Robust_params.dictionary'       # Constructs the path to the file where only the robust parameters are saved.\n",
    "\n",
    "        ### EXTRA: DEFINE PATH TO DATACOMPS!!!\n",
    "        datacomps_path = f'{res_dir}/[{i+1}]Data_comps.dictionary'\n",
    "\n",
    "        ### EXTRA: DEFINE PATH TO INFPARAMS!!!\n",
    "        inf_params_path = f'{res_dir}/[{i+1}]inf_params.dictionary'\n",
    "\n",
    "\n",
    "        if not os.path.exists(res_path):\n",
    "            with open(res_path, 'wb') as parameters:\n",
    "                pickle.dump(0, parameters)\n",
    "            \n",
    "            print(f'Running Model...') \n",
    "\n",
    "            seed = np.random.randint(0, 50)\n",
    "\n",
    "            rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "            start = time.time()                                      \n",
    "            \n",
    "            MCMCout = run_inference(models, args, rng_key, X, hypers)                                                                       \n",
    "            \n",
    "            mcmc_samples = MCMCout.get_samples()                          \n",
    "\n",
    "            mcmc_samples.update({'time_elapsed': (time.time() - start)/60}) \n",
    "            \n",
    "            pe = MCMCout.get_extra_fields()['potential_energy']                                                    \n",
    "            \n",
    "            mcmc_samples.update({'exp_logdensity': jnp.mean(-pe)})          \n",
    "\n",
    "            with open(res_path, 'wb') as parameters:                       \n",
    "                pickle.dump(mcmc_samples, parameters)\n",
    "\n",
    "                print('Inferred parameters saved.')\n",
    "        \n",
    "\n",
    "        if not os.path.exists(robparams_path) and os.stat(res_path).st_size > 5:\n",
    "\n",
    "            # Loading Inferred Parameters:\n",
    "            with open(res_path, 'rb') as parameters:\n",
    "                mcmc_samples = pickle.load(parameters)\n",
    "        \n",
    "            inf_params, data_comps = get_infparams(mcmc_samples, hypers, args)\n",
    "\n",
    "            # EXTRA: SAVE DATACOMPS !!!\n",
    "            with open(datacomps_path, 'wb') as parameters:\n",
    "                pickle.dump(data_comps, parameters)\n",
    "                print('\"data_comps\" SAVED.')\n",
    "\n",
    "            # EXTRA: SAVE INFPARAMS !!!\n",
    "            with open(inf_params_path, 'wb') as parameters:\n",
    "                pickle.dump(inf_params, parameters)\n",
    "                print('\"inf_params\" SAVED.')\n",
    "\n",
    "\n",
    "            if args.num_chains > 1:\n",
    "\n",
    "                thrs = {'cosineThr': 0.8, 'matchThr': 0.5}\n",
    "\n",
    "                # Finding Robust Components:\n",
    "                rob_params, X_rob, success = get_robustK(thrs, args, inf_params, data_comps)\n",
    "\n",
    "                # Saving Robust Data Components:\n",
    "                if success:\n",
    "                    \n",
    "                    rob_params.update({'sigma_inf': inf_params['sigma'], 'infX': X_rob})\n",
    "                   \n",
    "                    if 'sparseGFA' in args.model:                                        \n",
    "                    \n",
    "                        rob_params.update({'tauW_inf': inf_params['tauW']})\n",
    "                    \n",
    "                    with open(robparams_path, 'wb') as parameters:                          \n",
    "                        pickle.dump(rob_params, parameters)                                  \n",
    "                        print('Robust parameters saved')  \n",
    "                        \n",
    "                else:\n",
    "                    print('No robust components found => {i+1}]Robust_params.dictionary NOT CREATED')\n",
    "            \n",
    "\n",
    "            else:                                                                           # Handles the case with a single MCMC chain.\n",
    "\n",
    "                ### EXTRA: DEBUG\n",
    "                print(\"ENTERED ELSE BLOCK, TAKES THE AVERAGE OF THE SINGLE CHAIN\")\n",
    "\n",
    "                W = np.mean(inf_params['W'][0], axis=0)                                     # Averages the inferred W parameters across samples.\n",
    "            \n",
    "                Z = np.mean(inf_params['Z'][0], axis=0)                                     # Averages the inferred Z parameters across samples.\n",
    "                \n",
    "                X = [[np.dot(Z, W.T)]]                                                      # Computes Inferred X.\n",
    "                \n",
    "                rob_params = {'W': W, 'Z': Z, 'infX': X}                                    # Creates Robust Parameters Dictionary\n",
    "\n",
    "                with open(robparams_path, 'wb') as parameters:                         \n",
    "                    pickle.dump(rob_params, parameters)                              \n",
    "\n",
    "\n",
    "    ## Visualization\n",
    "\n",
    "    if 'synthetic' in args.dataset:                                                         \n",
    "        visualization.synthetic_data(res_dir, data, args, hypers)\n",
    "\n",
    "    else:\n",
    "        visualization.genfi(data, res_dir, args)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a0a1c27bf9fce2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset = 'genfi'\n",
    "    \n",
    "    if 'genfi' in dataset:\n",
    "        num_samples = 3000\n",
    "        K = 20\n",
    "        num_sources = 4\n",
    "        num_runs = 5\n",
    "        \n",
    "    else:\n",
    "        num_samples = 1500      \n",
    "        K = 5                  \n",
    "        num_sources = 3       \n",
    "        num_runs = 5\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\" Sparse GFA with reg. horseshoe priors\")\n",
    "\n",
    "    #parser.add_argument(\"model\", type=str, nargs=\"?\", default='GFA', help='add horseshoe prior over the latent variables')\n",
    "    parser.add_argument(\"model\", type=str, nargs=\"?\", default='sparseGFA', help='add horseshoe prior over the latent variables')\n",
    "\n",
    "    parser.add_argument(\"--num-samples\", nargs=\"?\", default=3000, type=int, help='number of MCMC samples')\n",
    "\n",
    "    parser.add_argument(\"--num-warmup\", nargs='?', default=2000, type=int, help='number of MCMC samples for warmup')\n",
    "\n",
    "    parser.add_argument(\"--K\", nargs='?', default=K, type=int, help='number of components')\n",
    "\n",
    "    parser.add_argument(\"--num-chains\", nargs='?', default=5, type=int, help= 'number of MCMC chains')\n",
    "\n",
    "    parser.add_argument(\"--num-sources\", nargs='?', default=num_sources, type=int, help='number of data sources')\n",
    "\n",
    "    parser.add_argument(\"--num-runs\", nargs='?', default=num_runs, type=int, help='number of runs')\n",
    "\n",
    "    parser.add_argument(\"--reghsZ\", nargs='?', default=False, type=bool)\n",
    "    #parser.add_argument(\"--reghsZ\", nargs='?', default=True, type=bool)\n",
    "\n",
    "    parser.add_argument(\"--percW\", nargs='?', default=42, type=int, help='percentage of relevant variables in each source')\n",
    "\n",
    "    parser.add_argument(\"--dataset\", nargs='?', default=dataset, type=str, help='choose dataset')\n",
    "\n",
    "    parser.add_argument(\"--device\", default='cpu', type=str, help='use \"cpu\" or \"gpu\".')\n",
    "\n",
    "    \"\"\"args = parser.parse_args()\"\"\"\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    numpyro.set_platform(args.device)\n",
    "    \n",
    "    numpyro.set_host_device_count(args.num_chains)\n",
    "    \n",
    "    main(args)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08bff06a866fc15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T01:48:32.321440Z",
     "start_time": "2024-08-24T01:48:31.710670Z"
    }
   },
   "id": "b1f86364307d08db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DEBUG"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T01:49:00.484755Z",
     "start_time": "2024-08-24T01:48:59.896928Z"
    }
   },
   "id": "4f90f43d08cd81e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Data_comps.dictionary (output of get_infparams)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4652df0c0b95fae1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "a list of shape (a, b, c, d), where \"a\" is the number of chains, and \"b\" is the number of factors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edcb08bf27f3973a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"/Users/mertenbiyaoglu/Desktop/ucl/thesis/codes/sGFA_AIDA/results/genfi/sparseGFA_K5_1chs_pW33_s50_reghsZ/[1]Data_comps.dictionary\"\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb1f2cff0eab9bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0a102b68e78f200"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_shape(data):\n",
    "    if isinstance(data, (list, np.ndarray)):\n",
    "        if isinstance(data, list):\n",
    "            data = [item for item in data if not isinstance(item, str)]\n",
    "        if len(data) == 0:\n",
    "            return (0,)\n",
    "        return (len(data),) + get_shape(data[0])\n",
    "    else:\n",
    "        return ()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a8390570fc113c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_shape(data)         # (1, 5, 83, 300)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e114f3ae72d4ef9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### inf_params.dictionary (output of get_infparams)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756851be03fa5ef0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"/Users/mertenbiyaoglu/Desktop/ucl/thesis/codes/sGFA_AIDA/results/genfi/sparseGFA_K5_1chs_pW33_s50_reghsZ/[1]inf_params.dictionary\"\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    inf_params = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f35ae07766a687be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(inf_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9558c909e446635"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inf_params['W']         # has numerical values\n",
    "inf_params['Z']         # has numerical values\n",
    "inf_params['sigma']     # has numerical values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d13af557a7876c42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_shape(data):\n",
    "    if isinstance(data, (list, np.ndarray)):\n",
    "        if isinstance(data, list):\n",
    "            data = [item for item in data if not isinstance(item, str)]\n",
    "        if len(data) == 0:\n",
    "            return (0,)\n",
    "        return (len(data),) + get_shape(data[0])\n",
    "    else:\n",
    "        return ()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afba9b6ab904c8f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_shape(inf_params['W'])          # (1, 50, 300, 5)\n",
    "get_shape(inf_params['Z'])          # (1, 50, 83, 5)\n",
    "get_shape(inf_params['sigma'])      # (50, 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "232d681860a6672a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model_params.dictionary (PRINTS OUT MCMC SAMPLES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "974042e879fb43ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A dictionary with the keys: 'W,' 'Z,' 'sigma,' 'time_elapsed,' 'exp_logdensity'\n",
    "MCMC samples are input for Data_comps and inf_params. W related stuff is not zero for mcmc samples but are zero for Data_comps and inf_params."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b66da3c3e4746722"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"/Users/mertenbiyaoglu/Desktop/ucl/thesis/codes/sGFA_AIDA/results/genfi/sparseGFA_K5_1chs_pW33_s50_reghsZ/[1]Model_params.dictionary\"\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    _data = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e7508c7f90fd1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98ca3751ffe3762e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mcmc_w = _data['W']\n",
    "mcmc_z = _data['Z']\n",
    "mcmc_s = _data['sigma']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73cce7329d1198cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = _data['W'].shape                        # (50, 300, 5)\n",
    "z = _data['Z'].shape                        # (50, 83, 5)\n",
    "s = _data['sigma'].shape                    # (50, 1, 5)\n",
    "t = _data['time_elapsed']\n",
    "l = _data['exp_logdensity']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bc42b9f59ad906b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Robust_params.dictionary (output of rob_params, X_rob, success = get_robustK(thrs, args, inf_params, data_comps))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb7c17881f79c7b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e4293f2ef3da661"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"/Users/mertenbiyaoglu/Desktop/ucl/thesis/codes/sGFA_AIDA/results/genfi/sfebqc_K20_5chs_pW20_s5000_CosThr8_MatThr5/[1]Robust_params.dictionary\"\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    rob_params = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aab2739307ef872"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(rob_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e981bb93dfdd8b3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rob_w = rob_params['W']             # has numerical values\n",
    "rob_z = rob_params['Z']             # has numerical values\n",
    "rob_infx = rob_params['infX']       # has numerical values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ce80782f0342ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rob_z_converted = rob_z.astype(float)\n",
    "df_rob_z = pd.DataFrame(rob_z_converted)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b24faa94d6bf8360"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "column_data = df_rob_z.iloc[:, 4]\n",
    "\n",
    "# Calculate the average of the absolute values for the first 14 values\n",
    "first_14_avg = column_data.iloc[:14].abs().mean()\n",
    "\n",
    "# Calculate the average of the absolute values for the remaining values\n",
    "remaining_avg = column_data.iloc[14:].abs().mean()\n",
    "\n",
    "# sorted_indices = df_rob_z.iloc[:, 4].abs().sort_values().index.tolist()\n",
    "sorted_indices = df_rob_z.iloc[:, 4].sort_values().index.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84d7973d28b63577"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"/Users/mertenbiyaoglu/Desktop/ucl/thesis/codes/sGFA_AIDA/aida_model/visit11_data_45subjs.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df_sorted = df.reindex(sorted_indices)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d34d4ae3c9ff8da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sorted = df_sorted.apply(lambda col: col.fillna(col.median()))\n",
    "\n",
    "normalized_data = data.apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=0)\n",
    "\n",
    "w = rob_params['W'].shape           # (300, 5)\n",
    "z = rob_params['Z'].shape           # (83, 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1fee7d1cec4d41f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_shape(data):\n",
    "    if isinstance(data, (list, np.ndarray)):\n",
    "        if isinstance(data, list):\n",
    "            data = [item for item in data if not isinstance(item, str)]\n",
    "        if len(data) == 0:\n",
    "            return (0,)\n",
    "        return (len(data),) + get_shape(data[0])\n",
    "    else:\n",
    "        return ()\n",
    "\n",
    "get_shape(rob_infx)  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e108c321781eb5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b35820867ec3ec57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspecting W for structural MRI and fMRI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d3a5bc495623e5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72d661ab09a9c84b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"/Users/mertenbiyaoglu/Desktop/ucl/thesis/codes/sGFA_AIDA/results/genfi/sparseGFA_K20_5chs_pW33_s3000_reghsZ/[3]Robust_params.dictionary\"\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    rob_params = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "757e3a54df71f6ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rob_w = rob_params['W']  \n",
    "\n",
    "rob_w_converted = rob_w.astype(float)\n",
    "rob_w = pd.DataFrame(rob_w_converted)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce7f24e6a52c2108"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### structural MRI "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d54581559ac2034d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smri = rob_w[0:69, 2]\n",
    "\n",
    "smri_con = smri.astype(float)\n",
    "smri = pd.DataFrame(smri_con)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fed9db4ab227591"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smri_2 = rob_w[0:69, 1]\n",
    "\n",
    "smri_con_2 = smri_2.astype(float)\n",
    "smri_2 = pd.DataFrame(smri_con_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aba3f67c87ea55a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_ = smri_2.iloc[:, 0].min().astype(float)\n",
    "max_ = smri_2.iloc[:, 0].max().astype(float)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4933921f39a337e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### positive fMRI "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fca306d0bc455774"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_fmri = rob_w[69:138, 5]\n",
    "\n",
    "p_fmri_con = p_fmri.astype(float)\n",
    "p_fmri = pd.DataFrame(p_fmri_con)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a09e1c8e67698f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### negative fMRI - 138:207"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "664c053b1d194607"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_fmri = rob_w[138:207, 1]\n",
    "\n",
    "n_fmri_con = n_fmri.astype(float)\n",
    "n_fmri = pd.DataFrame(n_fmri_con)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d551282bbfe900db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "structural MRI - 0:69\n",
    "positive fMRI - 69:138"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2619c3e41232e82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
